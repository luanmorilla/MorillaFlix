// api/openai.js ‚Äî vers√£o otimizada e resiliente
import OpenAI from "openai";

// Cache simples em mem√≥ria (reseta a cada rein√≠cio do server)
const cache = new Map();

export default async function handler(req, res) {
  if (req.method !== "POST") {
    return res.status(405).json({ error: "M√©todo n√£o permitido" });
  }

  try {
    const key = process.env.OPENAI_API_KEY;
    if (!key) {
      console.error("‚ùå OPENAI_API_KEY ausente.");
      return res.status(500).json({ error: "Chave da OpenAI n√£o configurada." });
    }

    const { prompt } = req.body;
    if (!prompt || typeof prompt !== "string") {
      return res.status(400).json({ error: "Prompt inv√°lido ou ausente." });
    }

    // üîπ Cache b√°sico para evitar requisi√ß√µes repetidas
    if (cache.has(prompt)) {
      return res.status(200).json({ result: cache.get(prompt) });
    }

    const client = new OpenAI({ apiKey: key });

    const completion = await client.chat.completions.create({
      model: "gpt-4-turbo", // tenta gpt-4-turbo (mais barato e r√°pido)
      messages: [
        {
          role: "system",
          content:
            "Voc√™ √© um especialista em cinema. Sua fun√ß√£o √© interpretar o pedido do usu√°rio e responder SOMENTE no formato: G√™nero1,G√™nero2,...|Filme ou S√©rie. N√£o explique nada al√©m disso.",
        },
        { role: "user", content: prompt },
      ],
      max_tokens: 30,
      temperature: 0.2,
    });

    let message =
      completion?.choices?.[0]?.message?.content?.trim() ||
      "A√ß√£o|Filme";

    // üîπ Garante formato v√°lido mesmo se o modelo responder errado
    if (!message.includes("|")) message = "A√ß√£o|Filme";
    message = message.replace(/[^\p{L}\p{N},| ]/gu, "").trim();

    cache.set(prompt, message); // salva no cache

    return res.status(200).json({ result: message });
  } catch (err) {
    console.error("‚ùå Erro na rota /api/openai:", err.message);

    // Fallback autom√°tico para GPT-3.5 se o GPT-4 falhar
    try {
      const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      const completion = await client.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "Responda SOMENTE no formato: G√™nero1,G√™nero2,...|Filme ou S√©rie.",
          },
          { role: "user", content: req.body.prompt },
        ],
        max_tokens: 30,
        temperature: 0.3,
      });
      const msg = completion?.choices?.[0]?.message?.content?.trim() || "A√ß√£o|Filme";
      return res.status(200).json({ result: msg });
    } catch (fallbackError) {
      console.error("‚ùå Fallback tamb√©m falhou:", fallbackError.message);
      return res.status(500).json({
        error: "Falha ao processar o prompt. Tente novamente em alguns segundos.",
      });
    }
  }
}
