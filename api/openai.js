import OpenAI from "openai";

export default async function handler(req, res) {
  // ğŸš« MÃ©todo invÃ¡lido
  if (req.method !== "POST") {
    return res.status(405).json({ error: "MÃ©todo nÃ£o permitido" });
  }

  try {
    // ğŸ§  Verifica se a variÃ¡vel estÃ¡ presente
    if (!process.env.OPENAI_API_KEY) {
      console.error("âŒ ERRO: OPENAI_API_KEY nÃ£o estÃ¡ definida no ambiente da Vercel.");
      return res.status(500).json({ error: "Chave da OpenAI ausente no servidor." });
    }

    // âœ… Inicializa o cliente OpenAI
    const client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    console.log("ğŸ“© Body recebido:", req.body);
    const { prompt } = req.body;

    // ğŸš« Prompt invÃ¡lido
    if (!prompt || typeof prompt !== "string") {
      console.warn("âŒ Prompt invÃ¡lido:", prompt);
      return res.status(400).json({ error: "Prompt invÃ¡lido" });
    }

    console.log("ğŸ¤– Enviando prompt para OpenAI:", prompt);

    // âœ¨ Chamada Ã  OpenAI
    const completion = await client.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: prompt }],
      max_tokens: 100,
      temperature: 0.7,
    });

    // ğŸ“ Resposta tratada
    const message = completion?.choices?.[0]?.message?.content || "Sem resposta da IA";
    console.log("âœ… Resposta da OpenAI:", message);

    return res.status(200).json({ result: message });

  } catch (error) {
    // ğŸš¨ Log detalhado para depuraÃ§Ã£o
    console.error("âŒ Erro detalhado na API OpenAI:", error);
    return res.status(500).json({
      error: error?.message || "Erro interno da IA",
      stack: error?.stack || "Sem stack trace disponÃ­vel",
    });
  }
}
